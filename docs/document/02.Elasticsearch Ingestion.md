# Document Data Ingestion via Elasticsearch

### **Features**

Elasticsearch incremental data into the hoodie. The environment is as follows：

（1）database

- Elasticsearch：7.3 

（2）library

- elasticsearch-spark-30_：7.12 （require）

- hadoop：2.10.x

【TIPS】 

（1）The library of elasticsearch-spark-30_ need to be specified through the --jar parameter when submitting the spark application.

（2）We recommend to use this hadoop version for maven dependency of 2.10.x. Exceptions may occur below this version.

### **Configuration**

| Configuration | Description                                                  | Level             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| --resource    | Elasticsearch resource location, where data is read and written to. | required          |
| --nodes       | List of Elasticsearch nodes to connect to.                   | required          |
| --port        | Default HTTP/REST port used for connecting to Elasticsearch - this setting is applied to the nodes in es. | required          |
| --props       | Path to properties file on localfs or dfs.                   | required          |
| --debug       | If you set debug mode, you can see a small amount of data in terminal. | optional（false） |

### *How to configure properties？*

The configuration is divided into the following three parts：

（1）**Read Elasticsearch extra optional parameter**

For the attachment parameters of es, please refer to ：[elasticsearch hadoop configuration](https://www.elastic.co/guide/en/elasticsearch/hadoop/7.3/configuration.html#configuration).

The configuration method in the resource file is '**hoodie.deltastreamer.es.extra.options.**' splicing with official native parameters.

（2）**Hoodie optional parameter**

The parameters imported into Hoodie are consistent with the official native parameters. Please refer to the official website.

（3）**transform for data ingestion**

Before the data is imported into Hoodie, simple ETL processing can be carried out through parameters of '**hoodie.deltastreamer.transformer.sql**'.

Here you can fill in a simple SQL syntax to preprocess the dataset.

### *FAQ*

（1）How to flatten nested data type？

You can refer to mongodb about flattening nested data type. If the automatic flattening cannot meet your expectations, you can set the parameter '**hoodie.deltastreamer.transformer.sql**' customize deployment strategy.

（2）Exception caused by data type od Array<>

The exception stack is as follows：

```tex
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] WARN  org.elasticsearch.spark.sql.ScalaRowValueReader  - Field 'space.disabledFeatures' is backed by an array but the associated Spark Schema does not reflect this;
              (use es.read.field.as.array.include/exclude) 
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] WARN  org.elasticsearch.spark.sql.ScalaRowValueReader  - Field 'space.disabledFeaturesArray' is backed by an array but the associated Spark Schema does not reflect this;
              (use es.read.field.as.array.include/exclude) 
[Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] ERROR org.apache.spark.executor.Executor  - Exception in task 0.0 in stage 0.0 (TID 0)
org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Field 'space.disabledFeaturesArray.y' not found; typically this occurs with arrays which are not mapped as single value
```

Field **space.disabledFeaturesArray** is an array type in es. All array types need to be specified through the '**es.read.field.as.array.include**' parameter. In this example, you need to add the following configuration：

```properties
es.read.field.as.array.include=space.disabledFeaturesArray
```

If more than one is separated by commas.

（3）Exception caused by duplicate column

The exception stack is as follows：

```tex
Exception in thread "main" org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema: `outflag`
```

This problem may be caused by Spark's insensitivity to column case，Mapping in ES is as follows：

```json
{
    "es_tp": {
        "mappings": {
            "properties": {
                "batchId": {
                    "type": "keyword"
                },
                ...
                "outFlag": {
                    "type": "keyword"
                },
                "outflag": {
                    "type": "long"
                },
                ...
            }
        }
    }
}
```

In es, outFlag and outflag are two different columns. At this time, you can avoid this problem by removing unused columns with parameter '**es.read.field.exclude**'.

```properties
es.read.field.exclude=outflag
```

（4）Time zone problem of extracting data

If there is a time zone problem when extracting data, you can turn off parameter 'es.mapping.date.rich', default value is true.

（5）How to get the keystore and truststore certificate paths

```
Caused by: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Cannot initialize SSL - Expected to find keystore file at [file:///home/workspace/certs/node-7.3.0.p12] but was unable to. Make sure that it is available on the classpath, or if not, that you have specified a valid URI.
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSSLContext(SSLSocketFactory.java:175)
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.getSSLContext(SSLSocketFactory.java:160)
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSocket(SSLSocketFactory.java:129)
  at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)
  at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)
  at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)
  at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)
  at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)
  at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.doExecute(CommonsHttpTransport.java:685)
  at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.access$200(CommonsHttpTransport.java:73)
  at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport$2.run(CommonsHttpTransport.java:659)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
  at org.elasticsearch.hadoop.mr.security.HadoopUser.doAs(HadoopUser.java:66)
  at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:656)
  at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:116)
  at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:432)
  at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428)
  at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:408)
  at org.elasticsearch.hadoop.rest.RestRepository.scroll(RestRepository.java:311)
  at org.elasticsearch.hadoop.rest.ScrollQuery.hasNext(ScrollQuery.java:93)
  at org.elasticsearch.spark.rdd.AbstractEsRDDIterator.hasNext(AbstractEsRDDIterator.scala:61)
  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:131)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Expected to find keystore file at [file:///home/workspace/certs/node-7.3.0.p12] but was unable to. Make sure that it is available on the classpath, or if not, that you have specified a valid URI.
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.loadKeyStore(SSLSocketFactory.java:195)
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.loadKeyManagers(SSLSocketFactory.java:215)
  at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSSLContext(SSLSocketFactory.java:173)
  ... 40 more
```

Place the keystore and truststore certificates into the zhongtai user of the node where the spark is located.

When running on Linux, the es.net.ssl.keystore.location and es.net.ssl.truststore.locatio paths are written file:/// full path.



（6）es.query only supports conditions in query

```
{"_source":["a","y"],"query":{"match_all":{}}}
```

_source is used to set what fields are returned in the query results, similar to specifying fields after the Select statement.

```
+---+---+------+------+
|a  |b  |x     |y     |
+---+---+------+------+
|1  |2  |111111|222222|
+---+---+------+------+
```

The conditions in the _source are not enforced.
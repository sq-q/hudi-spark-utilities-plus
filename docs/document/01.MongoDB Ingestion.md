# Document Data Ingestion via MongoDB

### **Features**

MongoDB incremental data into the hoodie. The environment is as follows：

（1）database

- MongoDB：4.0

（2）library

- mongo-spark-connector：3.0.1 （require）

- mongo-java-driver：3.12.10 （require）

【TIPS】 The above library need to be specified through the --jar parameter when submitting the spark application.


### **Configuration**

| Configuration | Description                                                  | Level             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| --uri         | The connection string in the form mongodb://host:port/.      | required          |
| --database    | The database name to read data from.                         | required          |
| --collection  | The collection name to read data from.                       | required          |
| --props       | Path to properties file on localfs or dfs.                   | required          |
| --debug       | If you set debug mode, you can see a small amount of data in terminal. | optional（false） |


### *How to configure properties？*

The configuration is divided into the following three parts：

（1）**Read MongoDB extra optional parameter** 

For the attachment parameters of mongodb, please refer to ：[mongodb spark-connector configuration](https://www.mongodb.com/docs/spark-connector/master/configuration/).

The configuration method in the resource file is '**hoodie.deltastreamer.mongodb.extra.options.**' splicing with official native parameters.

Of course, the two parameters of username and password do not need to be concatenated. See the FAQ below for more details

Some important configurations are as follows：

| Configuration | Description                                                  | Default Value             |
| ------------- | ------------------------------------------------------------ | ----------------- |
| user    | mongodb username |           |
| password       | mongodb password                   |           |
| batchSize        | The size of the internal batches within the cursor. |           |
| samplePoolSize       | The size of the pool to sample from when analyzing the schema.                   | 10000          |
| readPreference.name       | The name of the Read Preference mode to use. | Primary |

（2）**Hoodie optional parameter**

The parameters imported into Hoodie are consistent with the official native parameters. Please refer to the official website.

（3）**transform for data ingestion**

Before the data is imported into Hoodie, simple ETL processing can be carried out through parameters of '**hoodie.deltastreamer.transformer.sql**'. 

Here you can fill in a simple SQL syntax to preprocess the dataset.

### *FAQ*

（1）How to flatten nested data type？

When you set hoodie.deltastreamer.es.auto.flatten.enable is true，the embedded data type is automatically flattened to the leaf node. The main types are Array<Struct<>> and Struct<>. Refer to the following example:

```json
{
  "id": 1,
  "locs": [
    {
      "loc_id": "Axyxuii=",
      "x": 12.65,
      "y": 74.76
    },
    {
      "loc_id": "WWsexo=",
      "x": 32.15,
      "y": 18.19
    }
  ]
}
```

After flattening, the data is：

| id   | locs_loc_id | locs_x | locs_y |
| ---- | ----------- | ------ | ------ |
| 1    | Axyxuii=    | 12.65  | 74.76  |
| 1    | WWsexo=     | 32.15  | 18.19  |

It should be noted that，If Auto flattening is not enabled, it should be a piece of data，At this time, it is assumed that the set **hoodie key** to id. After automatic flattening is turned on, the data is theoretically two record. At this time, if the Hoodie key is not readjusted, the data may be lost! For example, in this example, we set hoodie key to id and locs_loc_id. **In other words, once the auto flattening function is turned on, it needs to be reconsidered hoodie key.**

Let's take another example,

```bson
{
    "_id" : ObjectId("6241579fa50aeed0adfbe58e"),
    "id" : 1.0,
    "locs" : [ 
        {
            "loc_id" : "Axyxuii=",
            "x" : 12.65,
            "y" : 74.76
        }, 
        {
            "loc_id" : "WWsexo=",
            "x" : 32.15,
            "y" : 18.19
        }
    ],
    "arr" : [ 
        {
            "id" : "Axyxuii=",
            "a" : 12.65,
            "b" : 74.76
        }, 
        {
            "id" : "WWsexo=",
            "a" : 32.15,
            "b" : 18.19
        }, 
        {
            "id" : "WWsexo=",
            "a" : 32.15,
            "b" : 18.19
        }
    ]
}
```

After flattening, the data is：

| id   | _id_oid                  | arr_id   | arr_a | arr_b | locs_loc_id | locs_x | locs_y |
| ---- | ------------------------ | -------- | ----- | ----- | ----------- | ------ | ------ |
| 1.0  | 6241579fa50aeed0adfbe58e | Axyxuii= | 12.65 | 74.76 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | Axyxuii= | 12.65 | 74.76 | WWsexo=     | 32.15  | 18.19  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | WWsexo=     | 32.15  | 18.19  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | Axyxuii=    | 12.65  | 74.76  |
| 1.0  | 6241579fa50aeed0adfbe58e | WWsexo=  | 32.15 | 18.19 | WWsexo=     | 32.15  | 18.19  |

This result may be correct or wrong, so whether automatic flattening can achieve the desired effect depends on the complexity of document metadata.

If the automatic flattening cannot meet your expectations, you can set the parameter '**hoodie.deltastreamer.transformer.sql**' customize deployment strategy.

（2）Authorization error for MongoClientURI accessing mongodb

The exception error is as follows:

```
com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=null, userName='username', source='admin', password=<hidden>, mechanismProperties={}}
```

Common reasons include:

 1.mongodb uri error

 2.mongodb enables authentication and verification (username and password are required at this time)

When the uri of mongodb is wrong, please check the format of the uri. The general format is as follows:

```
mongodb://host:port/
```

If there is a username and password, the uri format can be:

```
mongodb://username:password@host:port/databases
```

In mongodb, **different usernames and passwords can be set for different databases. The user name and password here must correspond to the database name, otherwise authorization errors are prone to occur.**

Of course, you can also configure the username and password in the parameters,the configuration is as follows:

```
user=xxx
password=xxxxx
```

[Access mongo report authorization error](https://www.jianshu.com/p/7657a275c422 /)

(3) Specifications for the use of pipes

The pipeline parameter configuration format is as follows:

```
pipeline= [{"$match": ...}]
```

The unrecognized pipeline error is as follows:

```
com.mongodb.MongoCommandException: Command failed with error 40324 (Location40324): 'Unrecognized pipeline stage name: 'createTime'' on server 172.16.10.72:27017. The full response is {"operationTime": {"$timestamp": {"t": 1648869779, "i": 1}}, "ok": 0.0, "errmsg": "Unrecognized pipeline stage name: 'createTime'", "code": 40324, "codeName": "Location40324", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1648869779, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "AAAAAAAAAAAAAAAAAAAAAAAAAAA=", "subType": "00"}}, "keyId": 0}}}
  at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:175)
  at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:302)
  at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:258)
```

The description for $match is:Filter the document stream to allow only matching documents to pass unmodified to the next pipeline stage. $match uses standard MongoDB queries. For each input document, output one document (match) or zero documents (no match).So the pipeline can only execute conditional queries.

[$match official website address](https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/)

[refer to: com.mongodb.spark.config.MongoInputConfig[232]](https://github.com/mongodb/mongo-spark/blob/3.0.x/src/main/scala/com/mongodb/spark/config/MongoInputConfig.scala)
